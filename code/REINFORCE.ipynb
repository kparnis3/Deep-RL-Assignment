{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kparnis3/Deep-RL-Assignment/blob/main/code/REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxxWBZioi0N"
      },
      "source": [
        "### Installation of packages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "apt-get install swig\n",
        "\n",
        "git clone https://github.com/pybox2d/pybox2d\n",
        "cd pybox2d\n",
        "python setup.py build\n",
        "python setup.py install\n",
        "\n",
        "apt-get install -y xvfb\n",
        "\n",
        "pip install \\\n",
        "    gym==0.21 \\\n",
        "    gym[box2d]==0.21 \\\n",
        "    pytorch-lightning==1.6.0 \\\n",
        "    optuna==2.7.0 \\\n",
        "    pyglet==1.5.27 \\\n",
        "    pyvirtualdisplay"
      ],
      "metadata": {
        "id": "oyQ7ov4M8pYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup of Virtual Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Packages used for Gym / Pytorch Lightning / Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gym \n",
        "import torch\n",
        "import random\n",
        "import statistics\n",
        "import optuna\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import Tensor, nn #create the Neural network\n",
        "from torch.utils.data import DataLoader #load training data\n",
        "from torch.utils.data.dataset import IterableDataset #Define where we get our data\n",
        "from torch.optim import AdamW #optimizer\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer \n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit, NormalizeObservation, NormalizeReward\n",
        "\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "     \n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu' # Run code on GPU (if possible)\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions to display video in notebook and test trained model"
      ],
      "metadata": {
        "id": "9V5v6jnxu8NY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_env(env_name, policy, obs_rms, **kwargs):\n",
        "  env = gym.make(env_name, **kwargs)\n",
        "  env = RecordVideo(env, 'videos', episode_trigger=lambda e: True)\n",
        "  env = NormalizeObservation(env)\n",
        "  env.obs_rms = obs_rms\n",
        "\n",
        "  for episode in range(10):\n",
        "    done = False\n",
        "    obs = env.reset()\n",
        "    while not done:\n",
        "      action = policy(obs).multinomial(1).cpu().item()\n",
        "      obs, _, done, _ = env.step(action)\n",
        "  del env\n",
        "\n",
        "\n",
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnk0wSWj0hAz"
      },
      "source": [
        "### Creating the gradient policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9a0b9cdnYtT"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, obs_size, out_dims,hidden_size=128):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear(obs_size, hidden_size)\n",
        "    self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.layer_final = nn.Linear(hidden_size, out_dims)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.tensor(x).float().to(device)\n",
        "    x = F.relu(self.layer1(x))\n",
        "    x = F.relu(self.layer2(x))\n",
        "    #x = torch.tanh(self.layer_final(x))\n",
        "    x = F.softmax(self.layer_final(x), dim=-1) #dim for par enviroments [[x1, x2, x3], [y1, y2, y3], ...] , softmax for probabilities \n",
        "\n",
        "    return x # [[p1, p2], [py1, py2] ....]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yvDC9qF0oPr"
      },
      "source": [
        "### Checking gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.vector.make('LunarLander-v2', num_envs=1)"
      ],
      "metadata": {
        "id": "Ry7ECCTdWB5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "id": "0Pbxh84fYK_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space, env.action_space"
      ],
      "metadata": {
        "id": "kjkGYTxUYLB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the environment"
      ],
      "metadata": {
        "id": "i3rKHfAaux7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_environment(env_name, num_envs):\n",
        "  env = gym.vector.make(env_name, num_envs, asynchronous=False)\n",
        "  env = RecordEpisodeStatistics(env) #Keep history of rewards from the env\n",
        "  env = NormalizeObservation(env)\n",
        "  env = NormalizeReward(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "nNixlpRxh15T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating the Dataset"
      ],
      "metadata": {
        "id": "A5L81w8HxVn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RLDataset(IterableDataset):\n",
        "  def __init__(self,env,policy,samples_per_epoch, gamma):\n",
        "    self.env = env\n",
        "    self.policy = policy\n",
        "    self.samples_per_epoch = samples_per_epoch\n",
        "    self.gamma = gamma #discount factor\n",
        "    self.obs = env.reset()\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def __iter__(self):\n",
        "    transitions = []\n",
        "    for step in range(self.samples_per_epoch):\n",
        "      action = self.policy(self.obs)\n",
        "      action = action.multinomial(1).cpu().numpy()\n",
        "      next_obs, reward, done, info = self.env.step(action.flatten())\n",
        "      transitions.append((self.obs, action, reward, done))\n",
        "      self.obs = next_obs\n",
        "    \n",
        "    obs_b, action_b, reward_b, done_b = map(np.stack, zip(*transitions))\n",
        "\n",
        "    running_return = np.zeros(self.env.num_envs, dtype=np.float32)\n",
        "    return_b = np.zeros_like(reward_b)\n",
        "\n",
        "    for row in range(self.samples_per_epoch -1, -1, -1): #from samples_per_epoch -1 -> -1 (moving backwards) (last to first)\n",
        "      running_return = reward_b[row] + (1 - done_b[row]) * self.gamma * running_return # return in each timestep in a signle backwards pass\n",
        "      return_b[row] = running_return\n",
        "\n",
        "    num_samples = self.samples_per_epoch * self.env.num_envs\n",
        "    obs_b = obs_b.reshape(num_samples, -1)\n",
        "    action_b = action_b.reshape(num_samples, -1)\n",
        "    return_b = return_b.reshape(num_samples, -1)\n",
        "\n",
        "    idx = list(range(num_samples))\n",
        "    random.shuffle(idx)\n",
        "\n",
        "    for i in idx:\n",
        "      yield obs_b[i], action_b[i], return_b[i]\n"
      ],
      "metadata": {
        "id": "92TmeOeqetcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgXi6A4Z1p75"
      },
      "source": [
        "#### The REINFORCE algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOmxUJ1vnY5d"
      },
      "outputs": [],
      "source": [
        "class Reinforce(LightningModule):\n",
        "  def __init__(self, env_name, num_envs=64, batch_size=256, hidden_size=64, policy_lr=1e-4, \n",
        "               samples_per_epoch=100, loss_fn=F.smooth_l1_loss,\n",
        "               gamma=0.99, optim=AdamW, entropy_coef=0.01):\n",
        "    super().__init__()\n",
        "    self.env = create_environment(env_name, num_envs=num_envs)\n",
        "    self.obs = self.env.reset()\n",
        "\n",
        "    obs_size = self.env.single_observation_space.shape[0] \n",
        "    action_dims =  self.env.single_action_space.n #discrete\n",
        "    self.policy = GradientPolicy(obs_size, action_dims, hidden_size) #create the policy\n",
        "\n",
        "    self.dataset = RLDataset(self.env, self.policy, samples_per_epoch, gamma)\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    policy_optimizer = self.hparams.optim(self.policy.parameters(), lr=self.hparams.policy_lr)\n",
        "    return policy_optimizer\n",
        "  \n",
        "  def train_dataloader(self): #specify how to get training data\n",
        "    return DataLoader(dataset=self.dataset, batch_size=self.hparams.batch_size)\n",
        "\n",
        "  def training_step(self, batch, batch_idx): #called twice, once with the actor another for critic\n",
        "    \n",
        "    obs_b, action_b, return_b = batch\n",
        "\n",
        "    prob_b = self.policy(obs_b)\n",
        "    log_prob_b = torch.log(prob_b + 1e-6) #calculate our log probabilities for our loss function\n",
        "    action_log_prob_b = log_prob_b.gather(1, action_b)\n",
        "\n",
        "    entropy = - torch.sum(prob_b * log_prob_b, dim =-1, keepdim=True)\n",
        "\n",
        "    pg_loss = - action_log_prob_b * return_b #minimize negative -> maximize\n",
        "    loss = (pg_loss - self.hparams.entropy_coef * entropy).mean()\n",
        "\n",
        "    self.log(\"episode/Policy Loss\", pg_loss.mean())\n",
        "    self.log(\"episode/Entropy\", entropy.mean())\n",
        "\n",
        "    return loss.mean()\n",
        "  \n",
        "  def training_epoch_end(self, training_step_outputs): #when a epoch ends\n",
        "    self.log(\"episode/Return\", self.env.return_queue[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mm9P0sX1wAA"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGQdpn0nY99"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8GdIwla1wrW"
      },
      "source": [
        "#### Training using REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig8c_RM8nZLN"
      },
      "outputs": [],
      "source": [
        "algorithm = Reinforce('LunarLander-v2')\n",
        "callback = EarlyStopping(\n",
        "    monitor='episode/Return',\n",
        "    patience=300,\n",
        "    strict=False,\n",
        "    verbose=1,\n",
        "    mode='max'\n",
        ")\n",
        "trainer = Trainer(gpus=num_gpus,\n",
        "                  max_epochs=5_000,\n",
        "                  log_every_n_steps=10,\n",
        "                  callbacks=[callback])\n",
        "trainer.fit(algorithm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "test_env('LunarLander-v2', algorithm.policy, algorithm.env.obs_rms)"
      ],
      "metadata": {
        "id": "pPV_0MwT_NvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_video(episode=9)"
      ],
      "metadata": {
        "id": "bBQmph67MUru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_video(episode=8)"
      ],
      "metadata": {
        "id": "fI5gGPPSywyo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "1_REINFORCE.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}